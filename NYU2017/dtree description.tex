Binary classifiers take the values of the input features $x_{i}$ and output $y_{i} \in \{0,1\}$. Decision trees do this by partitioning the feature space into subspaces, such that the divisions give rise to final regions with learned classifications. The subspaces divided at each step of the construction of the tree represent nodes of the tree, and the final set of subspaces after the desired number of partitions are created are the tree's terminal leaves.

These partitions can be complex, with many splits, leading to many nodes with high accuracy (the so-called ``purity'' of the leaves, determined by various measures). These trees are ``strong'' learners, but generally exhibit poor performance on unseen data in high dimensions since they are overfit on training data. Conversely, the partitions can be simple, with few splits (possibly even only one) having nodes with lower purity. These trees are called ``weak'' learners, but have the advantage of being simple and not overfitting the training data.

Robust against outliers and data transformations, decision trees are fast and their results are interpretable. In isolation, decision trees can perform well and have low bias, but they tend to exhibit high variance as errors in the first node quickly propogate through the children nodes of the tree when applied to data unseen by the model \cite{James_2013}. In order to reduce this variance, ensemble methods are frequently employed. We attempt to improve the performance of our models using two such techniques: Random Forest and Gradient Boosted Decision Trees. Both models are implemented in Python using packages scikit-learn \cite{scikit-learn} and xgboost \cite{Chen_2016}.